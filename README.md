# Anonymization of French legal cases

[![Build Status](https://travis-ci.com/ELS-RD/anonymisation.svg?token=9BHyni1rDpKLxVsHDRNp&branch=master)](https://travis-ci.com/ELS-RD/anonymisation)

The purpose of this projet is to apply `Named Entity Recognition` (`NER`) to extract specific information from legal cases like 
names, addresses, etc.  
These information will be used by anonymization system.  

## Scope

The only legal cases massively acquired by `ELS` not anonymized are those from appeal Courts.  
The database is called **Jurica**.  
The input data are XML files from **Jurica** as generated by `Temis` covering the period 2008-2018.

The project is focused on finding mentions of entities and guessing their types.  
It doesn't manage pseudo-anonymization (replacing entities text by initials), deciding what to hide, etc.  
For pseudo anonymization, replacin an entity name by its initials (or hashed initials) can be sufficient.  
It doesn't try to link variation of an entity name to the same entity.  

Deployment is not covered by this project.

> In the future, this project will be put in production.  
However, technical constrains (ex.: memory foot print, avoiding costly hardware) are already taken into account. 

## Challenges

Many `SOTA` algorithms are available as open source project.  
Therefore, developping a `NER` algorithm is not in the scope.  

The main focus of this work is to generate a **large high quality training set**, by:

- leveraging the extractions performed by `Temis`
- using some easy to describe patterns (with `regex`)
- looking for other occurrences of entities discovered with the other strategies
- building dictionaries of frequent names using all legal cases and look for them in each of them
- create some variation of the discovered entities and search for them (remove first or last name, change the case, etc.)
    - make the model more robust to error in the text
    - these variation can not be discovered easily with patterns
- extending any discovered to the neighbor words if it makes sense
    - should be done carefully otherwise there is a risk of lowering the quality of the training set 
- look for doubtful MWE candidates and declare them as doubtful
    - doubtful MWE candidates are any sequence of words starting with an up case
    - a filter is then applied to keep only those with a first name (based on a dictionary) 
    - no loss is computed on these entities
- find cities using a city dictionary
- removing paragraph containing 0 entity 
    - no entity paragraph may be due to an error in finding them 

> The purpose of ML is **to smooth the rules and the other tricks**, making the whole thing much more robust.  

## Type of tokens recognized

- Persons:
    - `PERS`: natural person *(include first name unlike `Temis`)*
    - `ORGANIZATION`: organization *(not done by `Temis`)*
- Lawyers:
    - `LAWYER`: lawyers *(not done by `Temis`)*
    - `BAR`: bar where lawyers are registered *(not done by `Temis`)* 
- Courts:
    - `COURT`: names of French courts *(not done by `Temis`)*
    - `JUDGE_CLERKS`: judges and court clerks *(not done by `Temis`)*
- Miscellaneous:
    - `ADDRESS`: addresses *(**very** badly done by `Temis`)*
    - `DATE`: any date, in numbers or letters *(not done by `Temis`)*

> Only taking care of `PERS` has been tried at first.  
It appeared that there was some issues with other entity types.  
Therefore, they have been added, greatly improving the quality of `PERS` recognition.
`BAR` and `DATE` where very easy to add and are useful for specific purpose.  

Type of entities to add in the future:

- phone numbers
- social security numbers
- credit card number
- RG

All the types to add may be managed by `regex`.

## Model

Main NER model is from [Spacy](https://spacy.io/) library and is best described in this [video](https://www.youtube.com/watch?v=sqDHBH9IjRU).
  
Basically it is a **CNN + HashingTrick / Bloom filter + L2S** approach over it.  
The L2S part is very similar to classical dependency parser algorithm (stack + actions).

#### Advantages:

- *no manual feature extraction* (done by `Spacy`: suffix and prefix, 3 letters each, and the word shape)
- quite *rapid on `CPU`*
- *low memory foot print* (for possible [Lambda deployment](https://github.com/ryfeus/lambda-packs))
- very *few dependencies*

Project is done in `Python` and can't be rewritten in something else because `Spacy` only exists in Python.  

> `Spark` on `Scala` in particular would be a bad choice for this project (`NLP` tool are very limited).

Other approaches (`Bi-LSTM`, etc.) may have shown slightly better performance but are much slower to train and during inference.  
For these reasons, and the specific need to run it over a `GPU` (costly option), they are not in our scope.

## Resources

No language related resource are used.

Few open data dictionary are used:

- a dictionary of French first names ([open data](https://opendata.paris.fr/explore/dataset/liste_des_prenoms_2004_a_2012/?disjunctive.annee&disjunctive.prenoms))
- a dictionary of postal code and cities of France ([open data](https://www.data.gouv.fr/fr/datasets/base-officielle-des-codes-postaux/))

> Both resources are stored on the Git repository (`resources/` folder).  
Both are not strategic to the success of the learning but provide a little help.

## Commands to use the code

* To learn the model

```python
python3 train.py
```

* To view `Spacy` results on a local web page ([`http://localhost:5000`](http://localhost:5000))

```python
python3 entities_viewer_spacy.py
```

* To view `Temis` results on a local web page ([`http://localhost:5000`](http://localhost:5000))

```python
python3 entity_viewer_temis.py
```

* To view differences with `Temis` (only for shared entity types)

```python
python3 display_errors.py
```

> All the project configuration is done through `resources/config.ini` file (mainly paths to resources).

### TODO:

- create a `Makefile` with train, create dataset, view Spacy, view Temis, view errors, run tests
- randomly change case of only several words in a MWE
- test if unknown entity match an existing one (A in B)
- Court formation
- social security : http://fr.wikipedia.org/wiki/Num%C3%A9ro_de_s%C3%A9curit%C3%A9_sociale_en_France#Signification_des_chiffres_du_NIR
 + https://github.com/ronanguilloux/IsoCodes/blob/master/src/IsoCodes/Insee.php
- credit card: (?:\d{4}-?){3}\d{4}
- search for phone number, etc.
- plaque immatriculation
- implement prediction with multi thread (pipe) V2.1 ? https://github.com/explosion/spaCy/issues/1530 
- Add rapporteurs / experts (close to word rapport)
- Birthday (né le ...) ?
- paste randomly the first word of a NER with the previous word to simulate recurrent errors
- dedupe entities : textacy.keyterms.aggregate_term_variants
- extract list of triplets textacy.extract.subject_verb_object_triples
- extract key sentences: textacy.keyterms.textrank
- extraires les acronymes et leurs defs textacy.extract.acronyms_and_definitions

Number of tags: 1773909
Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))
  0%|          | 0/145040.8 [00:00<?, ?it/s]
Iter 1
 10%|▉         | 14504/145040.8 [2:09:49<18:59:21,  1.91it/s]{'ner': 57.427544362485946}

Iter 2
 20%|██        | 29009/145040.8 [4:18:30<17:04:46,  1.89it/s]{'ner': 36.54777899100043}

Iter 3
 30%|███       | 43515/145040.8 [6:27:13<11:37:35,  2.43it/s]{'ner': 32.62351346588014}

Iter 4
 40%|████      | 58019/145040.8 [8:39:24<15:51:11,  1.52it/s]{'ner': 30.822936655417266}

Iter 5
 50%|█████     | 72525/145040.8 [10:52:50<9:14:25,  2.18it/s] {'ner': 29.70916408157069}

Iter 6
 60%|██████    | 87029/145040.8 [13:06:14<8:45:25,  1.84it/s]{'ner': 28.97088341355564}

Iter 7
 70%|███████   | 101534/145040.8 [15:19:42<6:31:44,  1.85it/s]{'ner': 28.445833063707028}

Iter 8
 80%|████████  | 116039/145040.8 [17:33:06<4:28:08,  1.80it/s]{'ner': 27.971498231318378}

Iter 9
 90%|█████████ | 130544/145040.8 [19:46:28<2:23:44,  1.68it/s]{'ner': 27.52150698761693}

--------------
Number of tags: 1775635
Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))

Iter 1
 50%|█████     | 14382/28763.26 [2:04:41<1:49:30,  2.19it/s]{'ner': 59.536253356406405}

Iter 2
100%|█████████▉| 28763/28763.26 [4:10:13<00:00,  1.89it/s]{'ner': 39.26536671542931}
28764it [4:10:13,  2.14it/s]        
-------------------
Number of tags: 1789948
Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))

Iter 1
 25%|██▍       | 14454/57816.04 [2:06:59<6:06:57,  1.97it/s]{'ner': 67.71795046884715}

Iter 2
 50%|█████     | 28909/57816.04 [4:13:19<4:05:11,  1.96it/s]{'ner': 46.69826582446979}

Iter 3
 75%|███████▌  | 43365/57816.04 [6:19:56<1:36:55,  2.49it/s]{'ner': 42.69678843677113}

Iter 4
57819it [8:29:44,  1.85it/s]{'ner': 40.70850695853477}
57820it [8:29:44,  2.42it/s]
----------------------
Number of tags: 1828754
Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))
  0%|          | 0/58912.84 [00:00<?, ?it/s]
Iter 1
 25%|██▍       | 14728/58912.84 [2:09:00<6:18:45,  1.94it/s]{'ner': 67.85554110441444}

Iter 2
 50%|█████     | 29458/58912.84 [4:21:36<3:27:16,  2.37it/s]{'ner': 46.97043271907819}

Iter 3
 75%|███████▌  | 44187/58912.84 [6:34:35<1:50:43,  2.22it/s]{'ner': 43.03420076370094}

Iter 4
58916it [8:51:00,  2.00it/s]
{'ner': 41.01037724554021}
-------
Number of tags: 1838488
Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))
  0%|          | 0/147282.1 [00:00<?, ?it/s]
Iter 1
 10%|▉         | 14728/147282.1 [2:08:14<18:56:38,  1.94it/s]{'ner': 66.24498462381916}

Iter 2
 20%|██        | 29458/147282.1 [4:17:05<14:30:56,  2.25it/s]{'ner': 45.65171872189785}

Iter 3
 30%|███       | 44187/147282.1 [6:26:10<12:32:49,  2.28it/s]{'ner': 41.7875151321731}

Iter 4
 40%|████      | 58915/147282.1 [8:38:55<13:11:23,  1.86it/s]{'ner': 39.87230896325718}

Iter 5
 50%|█████     | 73645/147282.1 [10:52:38<8:17:44,  2.47it/s] {'ner': 38.66143961688948}

Iter 6
 60%|██████    | 88373/147282.1 [13:06:16<8:45:27,  1.87it/s]{'ner': 37.84853459032075}

Iter 7
 70%|███████   | 103103/147282.1 [15:20:09<4:59:31,  2.46it/s]{'ner': 37.12548023300906}

Iter 8
 80%|████████  | 117832/147282.1 [17:33:47<3:32:43,  2.31it/s]{'ner': 36.63719058544939}

Iter 9
 90%|█████████ | 132560/147282.1 [19:47:40<2:16:39,  1.80it/s]{'ner': 36.07416604219725}

Iter 10
147289it [22:01:32,  1.91it/s]{'ner': 35.80839124857698}
147290it [22:01:32,  2.25it/s]
---------
Number of tags: 1838747
Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))
  0%|          | 0/58912.84 [00:00<?, ?it/s]
Iter 1
 25%|██▌       | 14729/58912.84 [2:07:57<5:11:57,  2.36it/s]{'ner': 67.09906184357169}

Iter 2
 50%|█████     | 29457/58912.84 [4:16:38<4:24:54,  1.85it/s]{'ner': 46.027869963762896}

Iter 3
 75%|███████▌  | 44187/58912.84 [6:25:35<1:47:29,  2.28it/s]{'ner': 42.25843731397822}

Iter 4
58915it [8:38:10,  1.88it/s]{'ner': 40.20154718467688}
58916it [8:38:10,  2.29it/s]
---------
Number of tags: 1855688
Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))

Iter 1
 33%|███▎      | 14750/44247.48 [2:10:24<3:34:44,  2.29it/s]{'ner': 71.92923828106768}

Iter 2
 67%|██████▋   | 29499/44247.48 [4:29:25<2:21:42,  1.73it/s]{'ner': 50.688311473414615}

Iter 3
44249it [7:00:41,  1.66it/s]{'ner': 46.80857206960354}
44250it [7:00:41,  2.13it/s]
---------
Number of tags: 2660741
Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))

Iter 1
 25%|██▌       | 17340/69358.24 [2:34:08<6:58:10,  2.07it/s]{'ner': 77.59002592418801}

Iter 2
 50%|█████     | 34680/69358.24 [5:07:29<4:10:34,  2.31it/s]{'ner': 52.346716683900695}

Iter 3
 75%|███████▌  | 52020/69358.24 [7:42:34<2:40:01,  1.81it/s]{'ner': 48.173801577489485}

Iter 4
69360it [10:21:42,  2.04it/s]
-------------
Number of tags: 2768352
Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))

  0%|          | 0/69481.48 [00:00<?, ?it/s]Iter 1
 25%|██▍       | 17370/69481.48 [2:44:06<7:58:20,  1.82it/s]{'ner': 77.94199584166518}

Iter 2
 50%|█████     | 34742/69481.48 [5:23:34<4:37:07,  2.09it/s]{'ner': 52.069788763078805}

Iter 3
 75%|███████▌  | 52113/69481.48 [8:04:13<2:08:17,  2.26it/s]{'ner': 47.722335816293025}

Iter 4
69483it [10:48:56,  1.78it/s]{'ner': 45.65050949805317}
69484it [10:48:56,  2.11it/s]
---------------
Learn NER model: 100%|██████████| 28635/28635 [41:59<00:00,  7.34 paragraphs/s]
Number of tags: 2849201
Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))

Iter 1
Learn NER model:  25%|██▍       | 17402/69612.0 [2:33:45<8:00:32,  1.81 paragraphs/s, loss: 79.66364140904534]
Iter 2
Learn NER model:  50%|█████     | 34806/69612.0 [5:10:56<5:33:31,  1.74 paragraphs/s, loss: 53.21245321840115] 
Iter 3
Learn NER model:  75%|███████▌  | 52209/69612.0 [7:49:55<2:35:10,  1.87 paragraphs/s, loss: 48.89310116812737] 
Iter 4
Learn NER model: 100%|██████████| 69612/69612.0 [10:32:52<00:00,  1.74 paragraphs/s, loss: 46.81073942351145]
---------------


Mot clés justice : http://www.justice.gouv.fr/_telechargement/mot_cle.csv


Ajouter un pattern d addresse:
BP 40122
