# Pseudo-anonymization of French legal cases

[![Build Status](https://travis-ci.com/ELS-RD/anonymisation.svg?token=9BHyni1rDpKLxVsHDRNp&branch=master)](https://travis-ci.com/ELS-RD/anonymisation)

The purpose of this projet is to apply `Named Entity Recognition` (`NER`) to extract specific information from legal cases like 
names, addresses, etc.  
These information will be used in a pseudo-anonymization system.  

## Scope

The only French legal cases massively acquired by `ELS` not pseudo-anonymized are those from appeal Courts.  
The database is called **Jurica**.  
The input data are XML files from **Jurica** as generated by `Temis` covering the period 2008-2018.

The project is focused on finding mentions of entities and guessing their types.  
It doesn't manage pseudo-anonymization (replacing entities text by initials), deciding what to hide, etc.  
For pseudo-anonymization, replacing an entity name by its initials (or hashed initials) can be sufficient.  
It doesn't try to link variation of an entity name to the same entity.  

Deployment is not covered by this project.

> In the future, this project will be put in production.  
However, technical constrains (ex.: memory foot print, avoiding costly hardware) are already taken into account. 

## Challenges

Many `SOTA` algorithms are available as open source project.  
Therefore, developing a `NER` algorithm is not in the scope.  

The main focus of this work is to generate a **large high quality training set**.  
Learning on a dataset only made with simple rules may only build a very weak model repeating the rules.  
Therefore, we have tried to include as many tricks as needed to catch / create complex pattern.  
With those, we have been able to produce a robust model, able to find much more entities than the rules which have been used to create the dataset.  

Below is listed of strategies used:


- `Temis`
    - leveraging the extractions performed by `Temis`
- Rules
    - using some easy to describe patterns to catch some entities (with `regex`)
    - find some other entities using dictionaries
- Name extension
    - extending any discovered entity to the neighbor words when it makes sense
        - done carefully otherwise there is a risk of lowering the quality of the training set 
- Find all occurrences of caught entities
    - looking for all occurrences of each entity already found in a document (2 pass process)
    - building dictionaries of frequent names using all documents and look for them in each of them (2 pass process)
- Dataset augmentation
    - Create some variation of the discovered entities and search for them 
        - By removing first or last name, change the case of one or more word in entity, remove key words (M., Mme, la société, ...), etc.
        - transformation are randomly applied (20% of entities are transformed)
        - make the model more robust to error in the text
        - these variations can not be discovered easily with patterns
            - eg. : changing the case is an easy way to workaround the creation of patterns to catch entities written in lower case
- Miscellaneous tricks
    - removing from train set all paragraphs containing 0 entity 
        - no entity paragraphs may be due to too simplistic patterns
    - Apply some priority rules depending of the source of the entity offset for cases where there is a conflict of type
        - some candidate generators are more safe than others 
        - a `_1` is added to the end of the tag label when it is safe and it is removed during the offset normalization step
    - Look for doubtful MWE candidates and declare them as doubtful
        - doubtful MWE candidates are any sequence of words starting with an up case
        - a filter is then applied to keep only those with a first name (based on a dictionary) 
        - no loss is computed on these entities, meaning they don't influence the model during training

> The purpose of ML is **to smooth the rules and the other tricks**, 
making the whole system much more robust to hard to catch entities. 
Data augmentation in particular has proved to be very efficient.  

## Recognized entity types

- Persons:
    - `PERS`: natural person *(include first name unlike `Temis`)*, **source**: `Temis` + name extension + other occurrences
    - `ORGANIZATION`: organization, **source**: `Temis` + rules + extension + other occurrences
    - `PHONE_NUMBER`: phone number, **source**: rules
    - `LICENCE_PLATE`: licence plate numbers, **source**: rules
- Lawyers:
    - `LAWYER`: lawyers, **source**: rules + other occurrences
    - `BAR`: bar where lawyers are registered *(not done by `Temis`)*, **source**: rules + other occurrences
- Courts:
    - `COURT`: names of French courts, **source**: rules + other occurrences
    - `JUDGE_CLERKS`: judges and court clerks, **source**: rules + other occurrences
- Miscellaneous:
    - `ADDRESS`: addresses *(**very** badly done by `Temis`)*, **source**: rules + other occurrences + dictionary
        - there is no way to always guess if the address owner is a `PERS` or an `ORGANIZATION`, therefore this aspect is not managed
    - `DATE`: any date, in numbers or letters, **source**: rules + other occurrences
    - `RG` : ID of the legal case, **source**: `Temis` + rules
    - `UNKNOWN` : only for train set, indicates that no loss should be apply on the word, whatever the prediction is, **source**: rules + dictionary

To each type, dataset augmentation and miscellaneous tricks have been applied.  
`Temis` only managed `PERS`, `ADDRESS` and `RG` types.  

In the future, French legislation may require to pseudo-anonymize following mentions in addition to those already known:

- first name of natural person
- judge name
- clerk name
- lawyer name

> Only taking care of `PERS` and `ADDRESS` entities has been tried at first.  
It appeared that there was some issues with the other entity types.  
Therefore, these entity types have been added, greatly improving the quality of `PERS` recognition.  
`RG`. `BAR` and `DATE` where very easy to add and are useful for some specific purposes.  

Type of entities that will not be included:

- social security numbers: Too few examples to learn from (3 numbers for 30 000 cases checked). Low risk.
- credit card number: not found in 30 000 cases, but lots of false positive. Low risk.

For both types of entity, there are lots of false positives.  
To limit these cases, we check the control number included in these Ids, but it's not enough to remove all false positives.  
Therefore, it seems smarter to not search for these Ids, moreover, they are quite hard to use for re-identification.

## Model

Main NER model is from [Spacy](https://spacy.io/) library and is best described in this [video](https://www.youtube.com/watch?v=sqDHBH9IjRU).
  
Basically it is a **CNN + HashingTrick / Bloom filter + L2S** approach over it.  
The L2S part is very similar to classical dependency parser algorithm (stack + actions).

### Advantages of the `Spacy` approach:

- *no manual feature extraction* (done by `Spacy`: suffix and prefix, 3 letters each, and the word shape)
- quite *rapid on `multicore CPU`* (to limit anonymization cost)
- *low memory foot print* (for possible [Lambda deployment](https://github.com/ryfeus/lambda-packs), to limit cost)
- off the shelf algorithm (documented, maintained, large community, third party lib, etc.)

Project is fully written in `Python` and can't be rewritten in something else because `Spacy` only exists on `Python`.  

> `Spark` on `Scala` in particular would be a bad choice for this project (`NLP` tools are limited).

Other approaches (`Bi-LSTM`, etc.) may have shown slightly better performances but are much slower to train and during inference.  
For these reasons, and the specific need to run it over a `GPU` (costly option), they are not in our scope.  
If available, `Spacy` can leverage `GPU`, however this option has not been explored.  

## Resources

No language related resource are used.

Few open data dictionary are used:

- a dictionary of French first names ([open data](https://opendata.paris.fr/explore/dataset/liste_des_prenoms_2004_a_2012/?disjunctive.annee&disjunctive.prenoms))
- a dictionary of postal code and cities of France ([open data](https://www.data.gouv.fr/fr/datasets/base-officielle-des-codes-postaux/))

> Both resources are stored on the Git repository (`resources/` folder).  
Both are not strategic to the success of the learning but provide a little help.

## Data and model paths

Paths listed below can be modified in the config file `resources/config.ini`.

### `XML`
- Cases have to be provided as XML in the format used by `Temis`.  
- One XML file represents one week of legal cases.  
- `XML` files should be put in folder `resources/training_data/`.  
- The case used for inference has to be placed in `resources/dev_data/`.  
- Folder `resources/test/` contains a `XML` used for unit tests.

### Other resources  
- Resources are to be put in folder `resources/courts`, `resources/postal_codes`, `resources/first_names`.  

### Model   
- Folder `resources/model/` will contain the `Spacy` model.


## Commands to use the code

This project uses [Python virtual environment](https://virtualenv.pypa.io/en/stable/) to manage dependencies without interfering with the those used by the machine.  
`pip3` and `python3` are the only requirements.  
To setup a virtual environment on the machine, install `virtualenv` from `pip3` and install the project dependencies (from the `requirements.txt` file).  

These steps are scripted in the `Makefile` (tested only on `Ubuntu`) and can be performed with the following command:  

```bash
make setup
```

> Variable `VIRT_ENV_FOLDER` can be changed in the `Makefile` to change where to install `Python` dependencies.

... then you can use the project by running one of the following actions:

* train a model

```bash
make train
```

* Find and export frequent entities (these entities are caught in all documents during the training set creation)

```bash
make export_frequent_entities
```

* view `Spacy` results on a local web page ([`http://localhost:5000`](http://localhost:5000))

```bash
make show_spacy_entities
```

* view `Temis` results on a local web page ([`http://localhost:5000`](http://localhost:5000))

```bash
make show_temis_entities
```

* view differences between Spacy and `Temis` (only for shared entity types)

```bash
make list_differences
```

* run unit tests

```bash
make test
```

> Most of the project configuration is done in `resources/config.ini` file.

## Setup Pycharm

For tests run from Pycharm, you need to create a Pytest test task.    
Then the working folder by default (implicit) is the test folder.  
**It has to be setup as the project root folder explicitly.**
